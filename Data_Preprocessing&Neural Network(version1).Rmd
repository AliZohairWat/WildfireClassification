---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

# Data Preprocessing

```{python}
#Load Libraries
import pandas as pd
import sqlite3 as sqlite3
import numpy as np
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler

```

```{python}
# Load the data into a DataFrame

con = sqlite3.connect("wildfire.sqlite")
fires = pd.read_sql_query(
    "select NWCG_REPORTING_AGENCY,CONT_DATE - DISCOVERY_DATE as CONT_TIME, \
    LONGITUDE,LATITUDE,OWNER_CODE,SOURCE_SYSTEM_TYPE,DISCOVERY_DATE,\
    DISCOVERY_DOY,STAT_CAUSE_DESCR,FIRE_SIZE from fires", con)
con.close()
```

```{python}
fires.info()
```

```{python}
fires = fires.drop_duplicates()
fires.shape
```

```{python}
fires = fires.dropna()
fires.shape
```

```{python}
fires1 = fires[(fires["STAT_CAUSE_DESCR"] != "Missing/Undefined") & (fires["STAT_CAUSE_DESCR"] != "Miscellaneous")]
fires1.shape
```

```{python}
fires1["STAT_CAUSE_DESCR"].value_counts().plot.bar()

```

```{python}
xFires=fires1.loc[:,fires1.columns != 'STAT_CAUSE_DESCR']
yFires=fires1['STAT_CAUSE_DESCR']
```

### One-hot-encode and Scale the Input

```{python}
from sklearn.compose import ColumnTransformer, make_column_selector
from sklearn.preprocessing import StandardScaler, OneHotEncoder
```

```{python}
# creat the column transformer(Look into Sparsity Later)
ct = ColumnTransformer([
    ('onehot',
        OneHotEncoder(drop="first"),
        make_column_selector(dtype_include=object)),
    ('scale',StandardScaler(),
        make_column_selector(dtype_include=np.number))],
    verbose_feature_names_out=False)

```

```{python}
xFires = ct.fit_transform(xFires)
ct.get_feature_names_out()
```

```{python}
#Training and test set split
xTrain,xTest,yTrain,yTest=train_test_split(xFires,yFires,\
                           test_size=0.1,random_state =441)

xTrain.shape
# yTrain.shape
```

### Oversampling and Undersampling to cope with imbalance classes

```{python}
counts = yTrain.value_counts()
counts
```

```{python}
from imblearn.over_sampling import RandomOverSampler

def count_under_10000(colname):
    if counts[colname] < 10000:
        return 10000
    return counts[colname]

#Perform undersampling
OverSampleRatio = {
    'Lightning' : count_under_10000('Lightning'), 'Debris Burning' : count_under_10000('Debris Burning'), 'Campfire' : count_under_10000('Campfire'), 
    'Equipment Use' : count_under_10000('Equipment Use'),    
    'Arson' : count_under_10000('Arson'), 'Children' : count_under_10000('Children'), 'Railroad' : count_under_10000('Railroad'),
    'Smoking' : count_under_10000('Smoking'), 'Powerline' : count_under_10000('Powerline'),
    'Fireworks' : count_under_10000('Fireworks'), 'Structure' : count_under_10000('Structure')
}

newSampStrat=RandomOverSampler(sampling_strategy=OverSampleRatio,random_state=441) #Goal balance all classes
xTrain,yTrain=newSampStrat.fit_resample(xTrain,yTrain) #perform the balancing newX and newY are balanced X and y
yTrain.value_counts().plot.bar()  #print result showing the nunmber of observation in each class
```

```{python}
counts = yTrain.value_counts()
counts
```

```{python}
from imblearn.under_sampling import RandomUnderSampler


newSampStrat=RandomUnderSampler(sampling_strategy='not minority',random_state=441) #Goal balance all classes
xTrain,yTrain=newSampStrat.fit_resample(xTrain,yTrain) #perform the balancing newX and newY are balanced X and y
yTrain.value_counts().plot.bar() 
```

```{python}
yTrain.value_counts()
```

```{python}
xTrain.shape
```

```{python}
# check that mean and variance of LONGITUDE is 0/1
print("mean(LONGITUDE) = {}, std(LONGITUDE) = {}".format(
    np.mean(xTrain[:,12]), 
    np.std(xTrain[:,12])))
```

### Label the response as numerical values using LabelEncoder

```{python}
#This is after over/under sampling since it will change classes to numeric values, then need to change code in over
# under sampling
from sklearn.preprocessing import LabelEncoder
# Encode the response "STAT_CAUSE_DESCR" (probability better just use the encoding from the orginal dataset)
# But want to learn to use sklearn.preprocessing.LabelEncoder
le=LabelEncoder()
yTrain=le.fit_transform(yTrain)
list(le.classes_)
```

```{python}
yTest=le.fit_transform(yTest)
list(le.classes_)
```

```{python}
# Separating the xTrain_trans further into train and validation data
xTrain,xVal,yTrain,yVal=train_test_split(xTrain, yTrain,\
                                           test_size=0.1,random_state=441)

```

### One-hot-encoding output for NN

```{python}
from tensorflow import keras
num_classes = 11
yTrain = keras.utils.to_categorical(yTrain, num_classes)
yVal = keras.utils.to_categorical(yVal, num_classes)
#yTest = keras.utils.to_categorical(yTest, num_classes)
```

# Building a Neural Network Model

```{python}
# #!pip install keras-tuner -q (already downloaded)
```

```{python}
import keras_tuner
from tensorflow.keras import layers
```

```{python active="", eval=FALSE}
Hyperparameters that could be tuned:
    number of layers,
    number of unit for each layer, 
    activation function of each layer, 
    learning_rate,
    loss? maybe
    dropout layer? (unknown)
    ...
                                        
```

```{python}
# Build model with hyperparameters
def build_model():
    model = keras.Sequential()
    
    #input layer
    model.add(layers.Dense(units=128, activation='relu', input_shape = (xTrain.shape[1],))) 
    
    #hidden layer(s)
    model.add(layers.Dense(units=128, activation="relu"))
    
    model.add(layers.Dense(units=128, activation="relu"))
    
    model.add(layers.Dense(units=128, activation="relu"))
    
    model.add(layers.Dense(units=128, activation="relu"))
    
    model.add(layers.Dense(units=128, activation="relu"))
    
    #Output Layer
    model.add(layers.Dense(11,activation='softmax'))
    
    model.compile(
        optimizer=keras.optimizers.Adam(learning_rate=0.0001),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )
    return model
                
model=build_model()
model.summary()
  
```

```{python}
# %%time
EPOCHS = 500

history = model.fit(
        xTrain, 
        yTrain,
        epochs=EPOCHS, 
        verbose=1,
        shuffle=True,
        validation_data = (xVal, yVal),
    )
```

```{python}
model.predict(xTest)
```

```{python}
predict_classes = np.argmax(model.predict(xTest), axis=1)
```

```{python}
print(f'Test score is: {sum(predict_classes==yTest)/len(yTest)}')
```
