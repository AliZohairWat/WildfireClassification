---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

```{python}
# # Need to limit number of cores used
# import os
# os.environ['MKL_NUM_THREADS'] = '1'
```

```{python}
#Load Libraries
import pandas as pd
import sqlite3 as sqlite3
import numpy as np
from matplotlib import pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.metrics import accuracy_score
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from imblearn.over_sampling import SMOTE
from sklearn.experimental import enable_iterative_imputer 
from sklearn.impute import IterativeImputer # MICE Imputation

from xgboost import XGBClassifier
import xgboost as xgb
```

### Data Preparation

```{python}
# Load the data into a DataFrame

con = sqlite3.connect("data/wildfire.sqlite")
fires = pd.read_sql_query(
    "select NWCG_REPORTING_AGENCY,CONT_DATE - DISCOVERY_DATE as CONT_DUR, \
    LONGITUDE,LATITUDE,OWNER_CODE,SOURCE_SYSTEM_TYPE,DISCOVERY_DATE,\
    DISCOVERY_DOY,STAT_CAUSE_DESCR,FIRE_SIZE from fires", con)
con.close()
```

```{python}
fires.info()
```

```{python}
fires = fires.drop_duplicates()
fires.shape
```

```{python}
fires1 = fires[(fires["STAT_CAUSE_DESCR"] != "Missing/Undefined") & (fires["STAT_CAUSE_DESCR"] != "Miscellaneous")]
fires1.shape
```

```{python}
fires1["STAT_CAUSE_DESCR"].value_counts().plot.bar()

```

```{python}
xFires=fires1.loc[:,fires1.columns != 'STAT_CAUSE_DESCR']
yFires=fires1['STAT_CAUSE_DESCR']

#Training and test set split
xTrain,xTest,yTrain,yTest=train_test_split(xFires,yFires,\
                           test_size=0.1,random_state =441)

xTrain.shape
# yTrain.shape
```

```{python}
counts = yTrain.value_counts()
counts
```

### MICE Imputation (Don't run)

```{python}
imp_mean.fit(xTrain)
```

```{python}
xTrain_impute=imp_mean.transform(xTrain)
xTest_impute=imp_mean.transform(xTest)
```

```{python}
train_impute_df=pd.DataFrame(xTrain_impute, columns=xTrain.columns)
train_impute_df['STAT_CAUSE_DESCR']=yTrain.reset_index(drop=True)
```

```{python}
test_impute_df=pd.DataFrame(xTest_impute, columns=xTest.columns)
test_impute_df['STAT_CAUSE_DESCR']=yTest.reset_index(drop=True)
```

```{python}
# To avoid re-imputing dataset everytime, training files were cached as csvs
train_impute_df.to_csv('imputed_training_data.csv', index=False)
test_impute_df.to_csv('imputed_test_data.csv', index=False)
```

## Test using SMOTE dataset (20 Apr 2023)

```{python}
# Refer to previous code for SMOTE implementation
# SMOTE is only applied to train and then saved as csv to avoid recomputing
train_smote_df = pd.read_csv("data/SMOTE_training_data.csv")
test_impute_df = pd.read_csv("data/imputed_test_data.csv")
```

```{python}
train_smote_df.info()
```

```{python}
test_impute_df.info()
```

```{python}
train_smote_df['STAT_CAUSE_DESCR'].value_counts().plot.bar()
```

### Random Under/Over Sampling to balance dataset
Dataset is made smaller for hyperparameter search due to computational and time restrictions

```{python}
# set up train and test spilt between x and y
xTrain = train_impute_df.drop(["STAT_CAUSE_DESCR"], axis = 1)
xTest = test_impute_df.drop(["STAT_CAUSE_DESCR"], axis = 1)
yTrain = train_impute_df[["STAT_CAUSE_DESCR"]]
yTest = test_impute_df[["STAT_CAUSE_DESCR"]]
```

```{python}
counts = yTrain.value_counts()
counts
```

```{python}
threshold = 3500

def count_under_threshold(colname):
    if counts[colname] < threshold:
        return threshold
    return counts[colname]

#Perform undersampling
OverSampleRatio = {
    'Lightning' : count_under_threshold('Lightning'), 'Debris Burning' : count_under_threshold('Debris Burning'), 'Campfire' : count_under_threshold('Campfire'), 
    'Equipment Use' : count_under_threshold('Equipment Use'),    
    'Arson' : count_under_threshold('Arson'), 'Children' : count_under_threshold('Children'), 'Railroad' : count_under_threshold('Railroad'),
    'Smoking' : count_under_threshold('Smoking'), 'Powerline' : count_under_threshold('Powerline'),
    'Fireworks' : count_under_threshold('Fireworks'), 'Structure' : count_under_threshold('Structure')
}

newSampStrat=RandomOverSampler(sampling_strategy=OverSampleRatio,random_state=441) #Goal balance all classes
xTrain,yTrain=newSampStrat.fit_resample(xTrain,yTrain) #perform the balancing newX and newY are balanced X and y
yTrain.value_counts().plot.bar()  #print result showing the nunmber of observation in each class
```

```{python}
# counts = yTrain.value_counts()
# counts
```

```{python}
from imblearn.under_sampling import RandomUnderSampler


newSampStrat=RandomUnderSampler(sampling_strategy='not minority',random_state=441) #Goal balance all classes
xTrain,yTrain=newSampStrat.fit_resample(xTrain,yTrain) #perform the balancing newX and newY are balanced X and y
yTrain.value_counts().plot.bar() 
```

```{python}
yTrain.value_counts()
```

### Feature Engineering

```{python}
train_smote_df.info()
```

```{python}
#list for cols to scale
cols_to_scale = ['CONT_DUR','LONGITUDE', 'LATITUDE', 'DISCOVERY_DATE', 'DISCOVERY_DOY', "FIRE_SIZE"]

#create and fit scaler
scaler = StandardScaler()
scaler.fit(train_smote_df[cols_to_scale])

#scale selected data
train_smote_df[cols_to_scale] = scaler.transform(train_smote_df[cols_to_scale])
```

```{python}
test_impute_df.info()
```

```{python}
#scale test data
test_impute_df[cols_to_scale] = scaler.transform(test_impute_df[cols_to_scale])
```

```{python}
# OHE for categorical variables
xTrain_model = pd.get_dummies(train_smote_df, columns = ["NWCG_REPORTING_AGENCY", "OWNER_CODE", "SOURCE_SYSTEM_TYPE"])
xTrain_model.info()
```

```{python}
xTest_model = test_impute_df.copy()
# xTest_model.info()
```

```{python}
# xTrain_model = xTrain.copy()
# xTest_model = xTest.copy()
```

```{python}
# keeping only same columns as xTrain
cols_to_keep = [col for col in xTest_model.columns if col in xTrain_model.columns]
xTest_model = xTest_model[cols_to_keep]
xTest_model.info()
```

```{python}
xTrain_model.info()
```

```{python}
# set up final x/y Train/Test datasets
xTrain = xTrain_model.drop(["STAT_CAUSE_DESCR"], axis = 1)
xTest = xTest_model.drop(["STAT_CAUSE_DESCR"], axis = 1)
yTrain = xTrain_model[["STAT_CAUSE_DESCR"]]
yTest = xTest_model[["STAT_CAUSE_DESCR"]]
```

```{python}
# Encode classes
lc = LabelEncoder() 

lc_yTrain = lc.fit_transform(yTrain) 
lc_yTrain

lc_yTest = lc.transform(yTest) 
lc_yTest = pd.Series(lc_yTest)
lc_yTest

```

### Model Training


### Hyperparameter tuning using grid search on subset of data

```{python}
params = {
        #'gamma': [0.3, 1],
        'max_depth': [40, 100],
        'n_estimators' : [200, 500],
       'learning_rate' : [0.01, 0.1, 0.9],
        'random_state' : [441],
        'reg_alpha' : [0.5, 0.8]
        }

```

```{python}
estimator = XGBClassifier(objective = "multi:softprob") 

grid_search = GridSearchCV(
    estimator=estimator,
    param_grid=params,
    scoring = 'accuracy',
    #n_jobs = ,
    cv = 4,
    verbose=3,
    refit = True
)

grid_search.fit(xTrain_model, lc_yTrain)
```

```{python}
grid_search.cv_results_
```

```{python}
print("The best training accuracy score is ", grid_search.best_score_ * 100, "%")
```

```{python}
print("The best parameter combination is:\n ", grid_search.best_params_)
```


### Final training model
Use the best hyperparameters from above to train on 30% of training data (~1million rows) due to computational constraints

```{python}
estimator = XGBClassifier(objective = "multi:softprob", max_depth = 100, n_estimators = 200,
                          random_state = 441, reg_alpha = 0.5) 

estimator.fit(xTrain.sample(frac = 0.3, axis = 'index', random_state = 441), pd.Series(lc_yTrain).sample(frac = 0.3, random_state = 441))
```

```{python active="", eval=FALSE}
final_model = grid_search.best_estimator_
```

### Prediction accuracy on final test set

```{python}
cols_order = estimator.get_booster().feature_names
xTest = xTest[cols_order]
```

```{python}
# Predict accuracy on the final test set
yPred = estimator.predict(xTest) 

yPred = [round(value) for value in yPred]

accuracy = accuracy_score(lc_yTest, yPred) 

print("Test Accuracy: %.2f%%" % (accuracy * 100.0))
```

```{python}
plt.barh(xTrain.columns, estimator.feature_importances_)
plt.title('Variable Importance for XGBoost Classifier')
```

```{python}
lc.classes_
```

```{python}
from sklearn.metrics import classification_report
print("\nClassification Report\n")
print(classification_report(lc_yTest, yPred, target_names = lc.classes_))
```
