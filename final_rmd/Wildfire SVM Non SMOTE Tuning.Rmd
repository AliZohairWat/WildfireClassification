---
jupyter:
  jupytext:
    formats: ipynb,Rmd
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.14.5
  kernelspec:
    display_name: Python 3
    language: python
    name: python3
---

```{python}
#Load Libraries
import pandas as pd
import numpy as np
import sqlite3 as sqlite3
import numpy as np
import pickle
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler
from imblearn.over_sampling import RandomOverSampler
from sklearn.metrics import accuracy_score
from sklearn import svm
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.multiclass import OneVsRestClassifier
```

```{python}
# Load the data into a DataFrame

con = sqlite3.connect("data/wildfire.sqlite")
fires = pd.read_sql_query(
    "select NWCG_REPORTING_AGENCY, CONT_DATE - DISCOVERY_DATE as CONT_TIME, CONT_DOY, \
    LONGITUDE, LATITUDE, SOURCE_SYSTEM_TYPE, DISCOVERY_DATE, FIRE_YEAR,\
    DISCOVERY_DOY, STAT_CAUSE_DESCR, FIRE_SIZE, OWNER_CODE from fires", con)
con.close()
```

```{python}
fires.info()
```

```{python}
fires = fires.drop_duplicates()
fires.shape
```

```{python}
fires = fires.dropna()
fires.shape
```

```{python}
fires1 = fires[(fires["STAT_CAUSE_DESCR"] != "Missing/Undefined") & (fires["STAT_CAUSE_DESCR"] != "Miscellaneous")]
fires1.shape
```

```{python}
# fires1["combined_date_dis"] = fires1["FIRE_YEAR"]*1000 + fires1["DISCOVERY_DOY"]
# fires1["combined_date_dis"] = pd.to_datetime(fires1["combined_date_dis"], format = "%Y%j")
# fires1["combined_date_dis"] = pd.to_datetime(fires1["combined_date_dis"])
# fires1["combined_date_con"] = fires1["FIRE_YEAR"]*1000 + fires1["CONT_DOY"]
# fires1["combined_date_con"] = pd.to_datetime(fires1["combined_date_con"], format = "%Y%j",errors="ignore")
# fires1["combined_date_con"] = pd.to_datetime(fires1["combined_date_con"],errors="coerce")
# fires1["week_day"] = fires1.combined_date_dis.dt.weekday
# fires1["month"] = fires1.combined_date_dis.dt.month
```

```{python}
# fires1=fires1.drop(['CONT_DOY', 'FIRE_YEAR', 'combined_date_dis', 'combined_date_con'],axis=1)
fires1=fires1.drop(['CONT_DOY', 'FIRE_YEAR'],axis=1)
fires1.info()
```

```{python}
xFires=fires1.loc[:,fires1.columns != 'STAT_CAUSE_DESCR']
yFires=fires1['STAT_CAUSE_DESCR']
xFires=pd.get_dummies(xFires, columns=['NWCG_REPORTING_AGENCY', 'SOURCE_SYSTEM_TYPE'])

#Training and test set split
xTrain,xTest,yTrain,yTest=train_test_split(xFires,yFires,\
                           test_size=0.1,random_state =441)

xTrain.shape
# yTrain.shape
```

```{python}
counts = yTrain.value_counts()
counts
```

```{python}
from imblearn.over_sampling import RandomOverSampler

def count_under_10000(colname):
    if counts[colname] < 10000:
        return 10000
    return counts[colname]

#Perform undersampling
OverSampleRatio = {
    'Lightning' : count_under_10000('Lightning'), 'Debris Burning' : count_under_10000('Debris Burning'), 'Campfire' : count_under_10000('Campfire'), 
    'Equipment Use' : count_under_10000('Equipment Use'),    
    'Arson' : count_under_10000('Arson'), 'Children' : count_under_10000('Children'), 'Railroad' : count_under_10000('Railroad'),
    'Smoking' : count_under_10000('Smoking'), 'Powerline' : count_under_10000('Powerline'),
    'Fireworks' : count_under_10000('Fireworks'), 'Structure' : count_under_10000('Structure')
}

newSampStrat=RandomOverSampler(sampling_strategy=OverSampleRatio,random_state=441) #Goal balance all classes
xTrain,yTrain=newSampStrat.fit_resample(xTrain,yTrain) #perform the balancing newX and newY are balanced X and y
```

```{python}
counts = yTrain.value_counts()
counts
```

```{python}
from imblearn.under_sampling import RandomUnderSampler


newSampStrat=RandomUnderSampler(sampling_strategy='not minority',random_state=441) #Goal balance all classes
xTrain,yTrain=newSampStrat.fit_resample(xTrain,yTrain) #perform the balancing newX and newY are balanced X and y
```

```{python}
yTrain.value_counts()
```

```{python}
# RBF kernel with covariate scaling
model_rbf = Pipeline(
    steps=[("scaler", StandardScaler()), 
           ("model", svm.SVC(kernel="rbf", gamma="scale", C=10000))]
)

# tuning parameter grid
# model__xyz specifies that parameter xyz is a parameter to model
param_grid = {
    "model__decision_function_shape": ["ovr", "ovo"],
}
# crossvalidation folds
cv = KFold(
    n_splits=5,  # number of folds
    shuffle=True # protects against data being ordered, e.g., all successes first
)

cv_rbf_onevall = GridSearchCV(
    estimator = model_rbf,
    param_grid = param_grid,
    cv = cv
)
```

```{python}
# %%time
cv_rbf_onevall.fit(X=xTrain, y=yTrain)
```

```{python}
cv_rbf_onevall.cv_results_
```

```{python}
final_model = cv_rbf_onevall.best_estimator_
```

```{python}
yPred = final_model.predict(xTest) 
```

```{python}
accuracy = accuracy_score(yTest, yPred)
print("Test Accuracy: %.2f%%" % (accuracy * 100.0))
```

```{python}
print("The best training accuracy score is ", cv_rbf_onevall.best_score_ * 100, "%")
```

```{python}
# %%time
filename='SVM_model_proposal'
pickle.dump(cv_rbf_onevall, open(filename, 'wb')) #Saving the model
```

```{python}
pickle.dump(yPred, open('predictions', 'wb'))
```

```{python}
pickle.load(open('predictions', 'rb'))
```

```{python}
yPred
```

```{python}

```
